{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96a0d5d9-ffa1-43b9-9524-7562307cbcbb",
   "metadata": {},
   "source": [
    "#### TASK 1 \n",
    "\n",
    "Youâ€™re given raw data that contains many flaws. \n",
    "Please find 5 different flaws in the data and fix them. Try to find the most important flaws that require to be fixed in order to enable forecasting (task 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7607178c-f9ce-4e53-a691-f5d19bb41eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2614b27e-7826-48cc-bc47-127dce2048c5",
   "metadata": {},
   "source": [
    "#### TASK 2\n",
    "\n",
    "Please describe (no code is needed) what will be the process required for forecasting the future number of active patients per country."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d546c417-1f65-4101-94a5-d1c761a6bb7a",
   "metadata": {},
   "source": [
    "Hence we have data saved in convenient way (one file for each country) for this task, I would like to try following approaches:\n",
    "- classical time series forecasting algorithms of family (S)AR(I)MA(X), using only target feature as an input and forecasting future cases with autoregressive modeling. From my experience these models work not so good on complex data (e.g. stocks) but may work good with this data. Of course, deep analysis should be done including trend/sesonality/stationarity checks. Based on this analysis we'll be able to understand what would be an optimal way of representing time series in autoregressive models.\n",
    "- jumping from time series task to a task of supervised learning - this is my favorite apporoach of forecasting time series and from my experience this works great on complex data. Let's stick to this method in following chapters.\n",
    "\n",
    "So the task is to represent data for supervised learning algorithm. We have to decide horison of forecasting - \n",
    "let's assume 1 for simplicity (means we'll forecast cases for the next day). \n",
    "I would like to describe 4 important points for this kind of modeling:\n",
    "- cross-validation strategy\n",
    "- target\n",
    "- features\n",
    "- models\n",
    "\n",
    "#### Target\n",
    "\n",
    "This is an important step to decide. However, we can experiment with different approaches here. I see following sane approaches:\n",
    "- value of Confirmed at step (t+1)\n",
    "- diff value of Confirmed at step (t+1): Confirmed(t+1) - Confirmed(t)\n",
    "- MA of Confirmed on some window at step (t+1)\n",
    "\n",
    "#### Cross-validation\n",
    "\n",
    "This is also an important step, as this should be a simulation of how model would perform in production. We can't use simple KFold for time series, better to use something more complicated - like roll-forward cross validation. I would like to try Time Series Split Cross-Validation or Blocked Cross-Validation.\n",
    "\n",
    "\n",
    "#### Features\n",
    "\n",
    "Features I see for this task are:\n",
    "- lag matrices of target (Confirmed)\n",
    "- diff matrices of target (Confirmed)\n",
    "- Moving averages (MA, EMA, etc.)\n",
    "- Features available for dataset: Deaths, Recovered, Active, Incident_Rate, Case_Fatality_Ratio with their aggregation on some window(s) and cumulative ratios where its sane to do so\n",
    "- Externally added features: general country info - wealth, GDP, total population, population density (use if fitting one-model-fits-all approach)\n",
    "- Handcrafted features like exp, pow(s) of a target, local predictions with simple models (like lin regression prediction from past week data) etc.\n",
    "- arrgegated features from neighboring countries (AVG number of cases, deaths, etc)\n",
    "\n",
    "No matter how much features we generate and use, we always have an ability to select best ones using:\n",
    "- feature importances (e.g. from RF model)\n",
    "- recursive feature elimination (or any other feature selection method of your choice)\n",
    "\n",
    "Important note here to consider: during feature generation we should be very careful to not impute future data to feature matrix (data leakege), as it will result into surprisingly great results during development but will fail to generalise at all.\n",
    "\n",
    "#### Models\n",
    "\n",
    "This is the most fascinating step, as it includes a lot of experimentation.\n",
    "\n",
    "I would like to stick to some classical ML algorithms (tree-based: RF, Gradient boosting; linear: regression, SVM) and also try to fit some DL models that fit ok to TS tasks (simple MLP, multi-head MLP to cover multi-task learning features; RNNs: vanilla RNN, LSTM, GRU; 1D-CNN; Attention-based transformers)\n",
    "\n",
    "But, again, following Occam's razor and Python principles: simple is better then complicated\n",
    "\n",
    "Training pipeline is simple: using described CV strategy: fit a model on traing split, evaluate it on validation split. Repeat for all splits. Record model results to some external file for further analysis and model selection.\n",
    "\n",
    "At the end, with best model(s) selected, we can tune hyperparamethers with some search algorithm (random search, grid search, Bayesian optimisation, genetic algorithm - I prefer using ray.tune or pygad for this)\n",
    "\n",
    "During evaluation it's important to track that model doesn't just repeat past value (lazy prediction - output Confirmed(t) as a prediction) and does not just learns mean value of series (again lazy model can just learn mean(Confirmed(0:t)))."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
